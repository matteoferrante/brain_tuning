{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data augmentations using encoding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import nilearn \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join as opj\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import glob\n",
    "from nilearn import plotting\n",
    "from nilearn.image import *\n",
    "import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nilearn.plotting import plot_stat_map\n",
    "from nilearn.image import mean_img\n",
    "from nilearn.plotting import plot_img, plot_epi\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import wandb\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset import fMRI_Dataset, fMRI_Text_Dataset\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_lightning.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from network import Encoder, ContrastiveModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import himalaya\n",
    "from himalaya.backend import set_backend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [07:03<00:00, 105.77s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "base_path=\"/home/matteo/brain-diffuser/data\"\n",
    "augment_imges_list=[]\n",
    "for sub in tqdm.tqdm([\"subj01\",\"subj02\",\"subj05\",\"subj07\"]):\n",
    "    stim_captions_train_path=opj(base_path,\"nsddata_stimuli\",\"stimuli\",\"nsd\",\"annotations\",f\"captions_train2017.json\")\n",
    "\n",
    "    processed_data=opj(base_path,\"processed_data\",sub)\n",
    "\n",
    "    sub_idx=int(sub.split(\"0\")[-1])\n",
    "\n",
    "    imgs_train_data=opj(processed_data,f\"nsd_train_stim_sub{sub_idx}.npy\")\n",
    "    augment_images = np.load(imgs_train_data)\n",
    "    augment_imges_list.append(augment_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_imges_list=np.concatenate(augment_imges_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/anaconda3/envs/borg/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPVisionModel(\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "      (position_embedding): Embedding(50, 768)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Use CLIPVision from huggingface to extract features from images\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel, AutoProcessor\n",
    "import torch\n",
    "\n",
    "# Load the CLIP model\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = AutoProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract image features using CLIP in batches\n",
    "\n",
    "def extract_image_features(images, model, processor, device, batch_size=128):\n",
    "\n",
    "    image_features = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.trange(0, len(images), batch_size):\n",
    "            batch_images = images[i:i + batch_size]\n",
    "\n",
    "            inputs = processor(images=batch_images, return_tensors=\"pt\",padding=True)[\"pixel_values\"].to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs).pooler_output\n",
    "                image_features.append(outputs)\n",
    "\n",
    "        image_features = torch.cat(image_features, dim=0)\n",
    "\n",
    "        return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 277/277 [03:32<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "augment_image_features = extract_image_features(augment_imges_list, model, processor, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_duplicate_indices(augment_embeddings, test_embeddings, threshold=0.95):\n",
    "    \"\"\"\n",
    "    Find indices of potential duplicates in augment_embeddings based on similarity with test_embeddings.\n",
    "\n",
    "    Args:\n",
    "        augment_embeddings (torch.Tensor): Tensor of shape (num_augment, embedding_dim) for augment images.\n",
    "        test_embeddings (torch.Tensor): Tensor of shape (num_test, embedding_dim) for test images.\n",
    "        threshold (float): Cosine similarity threshold above which images are considered duplicates.\n",
    "\n",
    "    Returns:\n",
    "        List[int]: Indices of potential duplicates in augment_embeddings.\n",
    "    \"\"\"\n",
    "    duplicate_indices = []\n",
    "    \n",
    "    # Calculate similarity between each augment_embedding and all test_embeddings\n",
    "    for i, augment_embedding in tqdm.tqdm(enumerate(augment_embeddings),total=len(augment_embeddings)):\n",
    "        similarity_scores = cosine_similarity(\n",
    "            augment_embedding.unsqueeze(0), test_embeddings  # Convert augment_embedding to (1, embedding_dim) for comparison\n",
    "        )\n",
    "        \n",
    "        # Check if any similarity score exceeds the threshold\n",
    "        if similarity_scores.max() > threshold:\n",
    "            duplicate_indices.append(i)\n",
    "    \n",
    "    return duplicate_indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_image_features = augment_image_features.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35436/35436 [01:21<00:00, 433.13it/s]\n",
      "100%|██████████| 35436/35436 [01:21<00:00, 436.88it/s]\n",
      "100%|██████████| 35436/35436 [01:19<00:00, 446.22it/s]\n",
      "100%|██████████| 35436/35436 [01:19<00:00, 448.27it/s]\n"
     ]
    }
   ],
   "source": [
    "indices_to_remove = []\n",
    "\n",
    "for sub in [\"CSI1\",\"CSI2\",\"CSI3\",\"CSI4\"]:\n",
    "\n",
    "    data_path =  f\"/home/matteo/storage/brain_tuning/{subj}\"\n",
    "    test_features = np.load(opj(data_path, \"test_image_features.npy\"))\n",
    "    duplicate_indices = find_duplicate_indices(augment_image_features, torch.tensor(test_features), threshold=0.95)\n",
    "\n",
    "    indices_to_remove.extend(duplicate_indices)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a clean feature tensor\n",
    "\n",
    "clean_augment_image_features = torch.cat([augment_image_features[i].unsqueeze(0) for i in range(len(augment_image_features)) if i not in indices_to_remove], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the clean tensor features and indices to remove here: /home/matteo/storage/brain_tuning/\n",
    "\n",
    "np.save(\"/home/matteo/storage/brain_tuning/clean_augment_image_features.npy\", clean_augment_image_features.numpy())\n",
    "np.save(\"/home/matteo/storage/brain_tuning/indices_to_remove.npy\", np.array(indices_to_remove))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_augment_image_features = np.load(\"/home/matteo/storage/brain_tuning/clean_augment_image_features.npy\")\n",
    "# indices_to_remove = np.load(\"/home/matteo/storage/brain_tuning/indices_to_remove.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Image encoding model to produce data augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import himalaya\n",
    "from himalaya.backend import set_backend\n",
    "\n",
    "\n",
    "## encode the augmented images using the image encoding model \n",
    "device_id = 0\n",
    "torch.cuda.set_device(device_id)  # Set the current device\n",
    "\n",
    "backend = set_backend(\"torch_cuda\")\n",
    "\n",
    "for subj in [\"CSI1\",\"CSI2\",\"CSI3\",\"CSI4\"]:\n",
    "    data_path =  f\"/home/matteo/storage/brain_tuning/{subj}\"\n",
    "    top_voxels = np.load(os.path.join(data_path, \"top_voxels.npy\"))\n",
    "\n",
    "    # Load the encoding model from the pickle file\n",
    "    with open(os.path.join(data_path, \"encoding_model.pkl\"), \"rb\") as f:\n",
    "        encoding_model = pickle.load(f)\n",
    "\n",
    "    augmented_brain = encoding_model.predict(backend.asarray(clean_augment_image_features).to(f'cuda:{device_id}'))[:, top_voxels]\n",
    "\n",
    "    np.save(os.path.join(data_path, \"augmented_brain.npy\"), augmented_brain.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save also augmented captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 112.99it/s]\n"
     ]
    }
   ],
   "source": [
    "base_path=\"/home/matteo/brain-diffuser/data\"\n",
    "augment_text_list=[]\n",
    "for sub in tqdm.tqdm([\"subj01\",\"subj02\",\"subj05\",\"subj07\"]):\n",
    "    stim_captions_train_path=os.path.join(base_path,\"nsddata_stimuli\",\"stimuli\",\"nsd\",\"annotations\",f\"captions_train2017.json\")\n",
    "    sub_idx=int(sub.split(\"0\")[-1])\n",
    "\n",
    "    processed_data=os.path.join(base_path,\"processed_data\",sub)\n",
    "    captions_train_data=os.path.join(processed_data, f\"nsd_train_cap_sub{sub_idx}.npy\")\n",
    "\n",
    "\n",
    "    augment_texts = np.load(captions_train_data,allow_pickle=True)\n",
    "    augment_text_list.append(augment_texts)\n",
    "\n",
    "augment_text_list=np.concatenate(augment_text_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices_to_remove\n",
    "clean_augment_text_list = np.array([augment_text_list[i] for i in range(len(augment_text_list)) if i not in indices_to_remove])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the clean augment text list here: /home/matteo/storage/brain_tuning/\n",
    "\n",
    "np.save(\"/home/matteo/storage/brain_tuning/clean_augment_text_list.npy\", clean_augment_text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matteo/anaconda3/envs/borg/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/matteo/anaconda3/envs/borg/lib/python3.8/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "# Load the CLIP text model and tokenizer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "text_model.to(device)\n",
    "text_model.eval()\n",
    "\n",
    "## Extract text features using CLIP in batches\n",
    "def extract_text_features(texts, text_model, text_tokenizer, device, batch_size=32):\n",
    "    text_features = []\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.trange(0, len(texts), batch_size):\n",
    "            batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "            # Tokenize and process the text batch\n",
    "            inputs = text_tokenizer(batch_texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "            outputs = text_model(**inputs).pooler_output  # Get the pooled output for the text features\n",
    "            \n",
    "            text_features.append(outputs)\n",
    "\n",
    "        # Concatenate all features along the batch dimension\n",
    "        text_features = torch.cat(text_features, dim=0)\n",
    "\n",
    "    return text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1108 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1108/1108 [00:14<00:00, 78.30it/s] \n"
     ]
    }
   ],
   "source": [
    "clean_augment_text_features = extract_text_features(clean_augment_text_list[:,0].tolist(), text_model, text_tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the clean text features here: /home/matteo/storage/brain_tuning/\n",
    "\n",
    "np.save(\"/home/matteo/storage/brain_tuning/clean_augment_text_features.npy\", clean_augment_text_features.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_augment_text_features = np.load(\"/home/matteo/storage/brain_tuning/clean_augment_text_features.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode augmented text features with text encoding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## encode the augmented images using the image encoding model \n",
    "device_id = 0\n",
    "torch.cuda.set_device(device_id)  # Set the current device\n",
    "\n",
    "backend = set_backend(\"torch_cuda\")\n",
    "\n",
    "for subj in [\"CSI1\",\"CSI2\",\"CSI3\",\"CSI4\"]:\n",
    "    data_path =  f\"/home/matteo/storage/brain_tuning/{subj}\"\n",
    "    top_voxels = np.load(os.path.join(data_path, \"TEXT_top_voxels.npy\"))\n",
    "\n",
    "    # Load the encoding model from the pickle file\n",
    "    with open(os.path.join(data_path, \"TEXT_encoding_model.pkl\"), \"rb\") as f:\n",
    "        encoding_model = pickle.load(f)\n",
    "\n",
    "    augmented_brain = encoding_model.predict(backend.asarray(clean_augment_text_features).to(f'cuda:{device_id}'))[:, top_voxels]\n",
    "\n",
    "    np.save(os.path.join(data_path, \"TEXT_augmented_brain.npy\"), augmented_brain.cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "braindiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
